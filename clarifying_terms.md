A clarification on terms -
* Normalization rescales the values into a range of [0,1]. This might be useful in some cases where all parameters need to have the same positive scale. However, the outliers from the data set are lost.
* Standardization rescales data to have a mean (μ) of 0 and standard deviation (σ) of 1 (unit variance). For most applications standardization is recommended.
* Regularization is a technique to avoid overfitting when training machine learning algorithms, which works by penalizing more complex models with more features.

Derivatives:
* Let’s say you have a graph of some function.
* If you take the first derivative, you get how the function changes. Positive numbers if function is increasing/slope is positive. Negative numbers if the function is decreasing/slope is negative. Where the first derivative is 0 means the slope of the function is 0 and we’re at either a peak or a valley in the original function.
* Second derivative - how the slope changes (i.e., how the changes in the function change) Positive numbers here if concave up (it’s at least a local minimum). Negative numbers here if it’s concave down (it’s a maximum).
